# Proyecto ETL de Criptomonedas con Airflow, PySpark y PostgreSQL

crypto-etl-pipeline es un pipeline ETL automatizado que extrae datos de criptomonedas desde la API de CoinGecko, los transforma con PySpark y los carga en PostgreSQL, todo orquestado con Apache Airflow.

## Estructura del Proyecto


```
crypto-etl-pipeline/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ etl_dag.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ extract_api.py
‚îÇ   ‚îú‚îÄ‚îÄ transform_spark.py
‚îÇ   ‚îî‚îÄ‚îÄ load_db.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```


## Tecnolog√≠as Usadas

- Ubuntu 20.04.6 LTS
- Python 3.8+
- Apache Airflow
- Apache Spark (PySpark)
- PostgreSQL
- CoinGecko API


## Ejecuci√≥n del Proyecto

1. Clona el repositorio.
2. Crea y activa un entorno virtual.

Primero se debe tener el terminal de Ubuntu. Si el sistema operativo que maneja es Windows, se puede descargar directamente de Microsoft Store. Una vez abierto el terminal, dirigirse a la carpeta del repositorio clonado y ejecutar en el terminal:

python -m venv venv
source venv/bin/activate

3. Instala las dependencias
Para instalar las dependencias necesarias, el proyecto tiene un archivo llamado requirements.txt que contiene todo lo necesario. Para instalarlo se debe ejecutar en el terminal el siguiente comando:

pip install -r requirements.txt


4. Configura PostgreSQL
Aseg√∫rate de que el servidor PostgreSQL est√© corriendo y tener presente las credenciales de la base de datos. se debe modificar el archivo load_db.py los siguientes elementos

            dbname='Your_DataBase',
            user='Your_User',
            password='Your_password',
            host='Your_host',
            port='Your_port'

Se debe crear la tabla llamada "coins" para el almacenamiento de informaci√≥n. Para esto se debe ejecutar en la base de datos el siguiente script:

CREATE TABLE coins (
    id TEXT PRIMARY KEY,
    name TEXT,
    symbol TEXT,
    current_price DOUBLE PRECISION,
    market_cap DOUBLE PRECISION
);


Adem√°s de la creaci√≥n de la tabla, se sugiere crear 2  indices para la b√∫squeda de nombre y el s√≠mbolo de la moneda como buenas practicas a la hora de consultar la tabla:

-- √çndice por el nombre de la criptomoneda: 
```
CREATE INDEX idx_coins_name ON coins(name);
```

-- √çndice por el s√≠mbolo de la criptomoneda: 
```
CREATE INDEX idx_coins_symbol ON coins(symbol);
```


5. Ejecuta Airflow
Se configura Airflow con sus credenciales de la siguiente manera:

```
export AIRFLOW_HOME=$(pwd)/airflow
airflow db init
airflow users create \
  --username admin \
  --firstname Your_name \
  --lastname your_lastname \
  --role Admin \
  --email admin@example.com \
  --password your_password
```

Una vez configurado, se debe tener 2 terminales para ejecutar 2 procesos de Airflow, uno la parte grafica web y la otra la funcional. Para eso ambos terminales de Ubuntu deben estar en la carpeta proyecto e iniciar el entorno virtual. Una vez dentro se debe ejecutar en el primer terminal el siguiente comando:

```
airflow scheduler
```

Y en el segundo terminal el siguiente comando:

```
airflow webserver --port 9090
```

6. Accede a la interfaz de Airflow
Ve a: http://localhost:9090

```
Usuario: Your_user

Contrase√±a: your_password
```

Activa el DAG llamado etl_dag y ejec√∫talo manualmente o espera su ejecuci√≥n autom√°tica diaria a las 12:00 PM (hora Colombia).

7. Visualizaci√≥n de los datos
Para ver el resultado del proyecto se puede generar una consulta sencilla en base de datos para ver el resultado

```
SELECT * FROM coins;
```
8. Ajuste de n√∫mero de datos
El proyecto consulta la API con el archivo extract_api.py, agrupa los datos en paquetes de 20 con la variable per_page y por √∫ltimo toma el n√∫mero de paquete 1 (los primeros 20 datos encontrados) con la variable page. Si se quiere codificar el n√∫mero de datos que se quiere quiere consultar a la API o que numero de paquete consultara, modificar estos valores num√©ricos.

üìà Frecuencia de Ejecuci√≥n
Este DAG se ejecuta diariamente a las 12:00 PM (hora Colombia). Si se desea cambiar la hora se debe ajustar en el archivo etl_dag.py en la variable schedule_interval (tener en cuenta la hora del servidor, ya que puede no coincidir con la hora local del usuario).

